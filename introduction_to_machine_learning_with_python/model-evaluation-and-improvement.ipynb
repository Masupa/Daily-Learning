{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # MODEL EVALUATION AND IMPROVEMENT\n",
    "    \n",
    "    This notebook aims to achieve the following:\n",
    "        * Demonstrate how to evaluate how well a model generalizes on new data\n",
    "        * Demonstrate how we can improve the performance of the model in generalizing\n",
    "          on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Model Evaluation\n",
    "    \n",
    "    How well is our model doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Let's build a model to classify the different species in the iris dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris  # Load iris dataset\n",
    "from sklearn.linear_model import LogisticRegression  # Log_Reg model\n",
    "\n",
    "# Instantiate iris dataset into features and ground truth labels\n",
    "features, labels = load_iris(return_X_y=True)\n",
    "\n",
    "# Instantiate Log-Reg Model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Classification\n",
      "==================== \n",
      "\n",
      "Predicted Class: setosa\n"
     ]
    }
   ],
   "source": [
    "# Well done... We've trained a Logistic Regression of the iris dataset.\n",
    "# Let's try it out\n",
    "\n",
    "# Use this data to test the new model\n",
    "sample_data = np.array([5.2, 3.0, 0.9, 0.4])\n",
    "class_prediction = log_reg.predict(sample_data.reshape(1, -1))\n",
    "\n",
    "print(\"Model Classification\")\n",
    "print(\"==================== \\n\")\n",
    "print(\"Predicted Class: {}\".format(load_iris().target_names[class_prediction][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    Wait a minute!\n",
    "    \n",
    "        * Can we trust this prediction?\n",
    "        * Do we know how well the model is doing in classifing the plant?\n",
    "        \n",
    "    What we have just done is the equivalence of teaching a particular subject to students, and allowing\n",
    "    them to graduate and apply the subject matter without testing howe well they are with the content :)\n",
    "    \n",
    "    Obviously, we want to test the students on preliminary work before allowing them to graduate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Split the data into train and test sets\n",
    "\n",
    "# Split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)  # Instantiate model\n",
    "log_reg.fit(X_train, y_train)  # Fit the model on train set\n",
    "\n",
    "acc_score = log_reg.score(X_test, y_test)  # Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Main Question: How well is our model doing?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "================\n",
      "\n",
      "Model Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Evaluation\\n================\")\n",
    "print(\"\\nModel Accuracy: {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    Well done!\n",
    "    \n",
    "    Now, we are confident that 97% of the time, our students will be able to handle the subject matter\n",
    "    correctly, when given a task in the real-world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cross Validation`\n",
    "\n",
    "    Evaluating how well our model generalizes on unseen data using the train_test_split method is a good idea,\n",
    "    but often, it is prone to the following:\n",
    "    \n",
    "        * Training set containing determinist features and testing set containing simple features; hence, model\n",
    "          produces a good result\n",
    "        * Training set containing unimportant features and testing set containing determinist features; hence,\n",
    "          model performs badly.\n",
    "          \n",
    "    Cross-validation aims to avoid the above two points by thoroughly training and evaluating the model on the\n",
    "    entire dataset.\n",
    "    \n",
    "    i) k-fold cross-validation\n",
    "        \n",
    "       This method splits the dataset into k equal parts, and repeatly trains k separate models with each part\n",
    "       assuming the testing set and the other k-1 parts assuming the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Student Notion`\n",
    "    \n",
    "    What we aim at doing with cross validation is evaluate our students on a defined number of exams (or tests)\n",
    "    rather than on one exam, to see how they perform on different kinds of assessments of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score  # Returns scores of models trained and tested using cv\n",
    "\n",
    "features, target = load_iris(return_X_y=True)\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model_acc_scores = cross_val_score(log_reg, features, target, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model accuracies: {}\".format(model_acc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To answer the question, \"how well is our model doing is generalizing on new data?\", we observe the\n",
    "    following using cross-validation:\n",
    "    \n",
    "        Reserving some parts of the data as test sets, \n",
    "        - the models produces an accuracy of 100%, in the best case\n",
    "        - and an accuracy of 93%, in the worst case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG Performance\n",
    "print(\"Expected AVG Performance\")\n",
    "print(\"========================\")\n",
    "print(\"AVG Score: {}\".format(np.mean(model_acc_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # MODEL IMPROVEMENT\n",
    "    \n",
    "    With model improvement, we are more concerned with finding the model parameters that boost the model's\n",
    "    performance. Once such technique that this notebook covers is GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
